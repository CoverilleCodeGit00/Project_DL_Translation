{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e28cebba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\benic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\benic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\benic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Racine : C:\\Users\\benic\\Documents\\Projet_DL_Translation\n",
      "Device : cpu\n"
     ]
    }
   ],
   "source": [
    "# Cellule 1 : Configuration et chemins locaux\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 1. Configuration du chemin racine\n",
    "ROOT_DIR = Path(\"..\").resolve()\n",
    "if str(ROOT_DIR) not in sys.path:\n",
    "    sys.path.append(str(ROOT_DIR))\n",
    "\n",
    "# 2. Rechargement auto\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# 3. Dossiers\n",
    "MODELS_DIR = ROOT_DIR / 'models' / 'nllb_local'\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Racine : {ROOT_DIR}\")\n",
    "print(f\"Device : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc4123ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benic\\Documents\\Projet_DL_Translation\\venv_dl\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\benic\\Documents\\Projet_DL_Translation\\venv_dl\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\benic\\.cache\\huggingface\\hub\\models--facebook--nllb-200-distilled-600M. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 512/512 [00:01<00:00, 428.66it/s, Materializing param=model.shared.weight]                                   \n",
      "The tied weights mapping and config for this model specifies to tie model.shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie model.shared.weight to model.decoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie model.shared.weight to model.encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    }
   ],
   "source": [
    "# Cellule 2 : Initialisation\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "MODEL_CHECKPOINT = \"facebook/nllb-200-distilled-600M\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT, src_lang=\"fra_Latn\", tgt_lang=\"eng_Latn\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "037c7db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du set train reduite a : 3005 phrases.\n",
      "Taille du set validation reduite a : 103 phrases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3005/3005 [00:00<00:00, 12241.72 examples/s]\n",
      "Map: 100%|██████████| 103/103 [00:00<00:00, 5649.52 examples/s]\n",
      "Map: 100%|██████████| 1058/1058 [00:00<00:00, 13284.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Cellule 3 : Ingestion et Sous-echantillonnage 10%\n",
    "from datasets import load_dataset\n",
    "from src.hf_utils import preprocess_function\n",
    "\n",
    "data_files = {\n",
    "    \"train\": str(ROOT_DIR / 'data' / 'processed' / 'train_nmt_fr_en.csv'),\n",
    "    \"validation\": str(ROOT_DIR / 'data' / 'processed' / 'valid_nmt_fr_en.csv'),\n",
    "    \"test\": str(ROOT_DIR / 'data' / 'processed' / 'test_nmt_fr_en.csv')\n",
    "}\n",
    "\n",
    "# Chargement complet\n",
    "raw_datasets = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "# --- REDUCTION A 10% POUR LE TEST LOCAL ---\n",
    "for split in [\"train\", \"validation\"]:\n",
    "    raw_datasets[split] = raw_datasets[split].shuffle(seed=42).select(range(int(len(raw_datasets[split]) * 0.1)))\n",
    "    print(f\"Taille du set {split} reduite a : {len(raw_datasets[split])} phrases.\")\n",
    "\n",
    "# Tokenization\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    lambda x: preprocess_function(x, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c691d383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demarrage du Fine-Tuning local (test 10%)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benic\\Documents\\Projet_DL_Translation\\venv_dl\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='668' max='1503' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 668/1503 2:14:08 < 2:48:10, 0.08 it/s, Epoch 0.44/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cellule 4 : Entrainement local avec contournement du bug tokenizer\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "from src.hf_utils import compute_metrics\n",
    "\n",
    "# Le DataCollator a deja le tokenizer, c'est lui qui gere le padding\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(MODELS_DIR),\n",
    "    eval_strategy=\"epoch\",       \n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2, \n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=1,            \n",
    "    predict_with_generate=True,\n",
    "    fp16=torch.cuda.is_available(), \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    report_to=\"none\"               \n",
    ")\n",
    "\n",
    "# On retire l'argument 'tokenizer' qui pose probleme dans ton environnement\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda p: compute_metrics(p, tokenizer)\n",
    ")\n",
    "\n",
    "print(\"Demarrage du Fine-Tuning local (test 10%)...\")\n",
    "trainer.train()\n",
    "\n",
    "# Sauvegarde manuelle du tokenizer puisque le Trainer ne l'a pas fait\n",
    "tokenizer.save_pretrained(str(MODELS_DIR / \"final_model\"))\n",
    "trainer.save_model(str(MODELS_DIR / \"final_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a10581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 5 : Evaluation finale (Inference)\n",
    "from tqdm import tqdm\n",
    "\n",
    "model.eval()\n",
    "df_test = pd.read_csv(data_files[\"test\"]).sample(50) # On teste sur 50 phrases seulement en local\n",
    "predictions = []\n",
    "forced_bos_token_id = tokenizer.lang_code_to_id[\"eng_Latn\"]\n",
    "\n",
    "for text in tqdm(df_test['text_fr'].tolist()):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id, max_length=128)\n",
    "    predictions.append(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0])\n",
    "\n",
    "df_test['prediction'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a287d6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 6 : Tableau comparatif\n",
    "# Vous pouvez relancer cette cellule autant de fois que vous voulez pour voir d'autres exemples\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df_test[['text_fr', 'text_en', 'prediction']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78928895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 7 : Preuve mathematique de la limite du modele Vanilla\n",
    "# Calcul des longueurs des phrases sources\n",
    "df_test['src_len'] = df_test['text_fr'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "df_short = df_test[df_test['src_len'] < 10]\n",
    "df_long = df_test[df_test['src_len'] > 25]\n",
    "\n",
    "def compute_bleu(df):\n",
    "    if len(df) == 0: return 0.0\n",
    "    preds = df['prediction'].tolist()\n",
    "    # SacreBLEU attend une liste de listes pour les references\n",
    "    refs = [df['text_en'].tolist()]\n",
    "    return sacrebleu.corpus_bleu(preds, refs).score\n",
    "\n",
    "print(f\"--- ANALYSE DE ROBUSTESSE ---\")\n",
    "print(f\"BLEU Global : {compute_bleu(df_test):.2f}\")\n",
    "print(f\"BLEU Phrases Courtes (<10 mots) : {compute_bleu(df_short):.2f} (Support: {len(df_short)})\")\n",
    "print(f\"BLEU Phrases Longues (>25 mots) : {compute_bleu(df_long):.2f} (Support: {len(df_long)})\")\n",
    "print(\"\\nConclusion: L'effondrement du score sur les longues phrases demontre la perte d'information dans le vecteur de contexte unique.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_dl (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
