{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ee34bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Racine du projet : C:\\Users\\benic\\Documents\\Projet_DL_Translation\n",
      "Device utilise : cpu\n"
     ]
    }
   ],
   "source": [
    "# Cellule 1 : Configuration initiale et imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sacrebleu\n",
    "\n",
    "# 1. Configuration robuste du chemin racine\n",
    "ROOT_DIR = Path(\"..\").resolve()\n",
    "if str(ROOT_DIR) not in sys.path:\n",
    "    sys.path.append(str(ROOT_DIR))\n",
    "\n",
    "# 2. Rechargement automatique des modules src/\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# 3. Imports de notre backend\n",
    "from src.translation_models import Encoder, Decoder, Seq2SeqVanilla\n",
    "from src.translation_utils import train_sentencepiece_models, TranslationDataset, collate_fn_translation\n",
    "from src.train_utils import train_model\n",
    "\n",
    "# 4. Definition du hardware\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Racine du projet : {ROOT_DIR}\")\n",
    "print(f\"Device utilise : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39e4aa09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation des dictionnaires de sous-mots...\n",
      "Chargement des donnees d'entrainement depuis C:\\Users\\benic\\Documents\\Projet_DL_Translation\\data\\processed\\train_nmt_fr_en.csv\n",
      "Entrainement du modele SentencePiece FR...\n",
      "Entrainement du modele SentencePiece EN...\n",
      "Entrainement SentencePiece termine avec succes. Fichiers sauvegardes dans : C:\\Users\\benic\\Documents\\Projet_DL_Translation\\models\n"
     ]
    }
   ],
   "source": [
    "# Cellule 2 : Entrainement des Tokenizers (BPE)\n",
    "# Attention : Cette etape ne doit voir QUE le set de Train !\n",
    "train_csv_path = str(ROOT_DIR / 'data' / 'processed' / 'train_nmt_fr_en.csv')\n",
    "models_dir_path = str(ROOT_DIR / 'models')\n",
    "\n",
    "print(\"Creation des dictionnaires de sous-mots...\")\n",
    "train_sentencepiece_models(train_csv_path, vocab_size=8000, model_dir=models_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87060408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset reduit a 20.0% : 6011 phrases chargees.\n",
      "Dataset reduit a 20.0% : 207 phrases chargees.\n",
      "Nombre de batches pour l'entrainement (20%) : 188\n",
      "Nombre de batches pour la validation (20%) : 7\n"
     ]
    }
   ],
   "source": [
    "# Cellule 3 : Chargement des Datasets (MODE TEST 20%)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "spm_fr_path = str(ROOT_DIR / 'models' / 'spm_fr.model')\n",
    "spm_en_path = str(ROOT_DIR / 'models' / 'spm_en.model')\n",
    "\n",
    "valid_csv_path = str(ROOT_DIR / 'data' / 'processed' / 'valid_nmt_fr_en.csv')\n",
    "\n",
    "# On ajoute l'argument sample_frac=0.2 pour ne prendre que 20% des CSV\n",
    "train_data = TranslationDataset(train_csv_path, spm_fr_path, spm_en_path, sample_frac=0.2)\n",
    "valid_data = TranslationDataset(valid_csv_path, spm_fr_path, spm_en_path, sample_frac=0.2)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_iterator = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn_translation)\n",
    "valid_iterator = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn_translation)\n",
    "\n",
    "print(f\"Nombre de batches pour l'entrainement (20%) : {len(train_iterator)}\")\n",
    "print(f\"Nombre de batches pour la validation (20%) : {len(valid_iterator)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "438eb81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demarrage de l'entrainement pour 10 epoques...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoque: 01 | Temps: 7.0m 47s => Checkpoint sauvegarde !\n",
      "\tTrain Loss: 6.527 | Train PPL: 683.532\n",
      "\t Val. Loss: 6.382 |  Val. PPL: 591.096\n",
      "\tMetriques Val -> BLEU: 1.52 | chrF: 4.80 | METEOR: 0.00\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoque: 02 | Temps: 7.0m 27s \n",
      "\tTrain Loss: 6.123 | Train PPL: 456.235\n",
      "\t Val. Loss: 6.440 |  Val. PPL: 626.490\n",
      "\tMetriques Val -> BLEU: 7.15 | chrF: 10.17 | METEOR: 0.00\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoque: 03 | Temps: 7.0m 48s \n",
      "\tTrain Loss: 5.986 | Train PPL: 397.930\n",
      "\t Val. Loss: 6.444 |  Val. PPL: 628.853\n",
      "\tMetriques Val -> BLEU: 15.11 | chrF: 15.87 | METEOR: 2.26\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoque: 04 | Temps: 7.0m 44s \n",
      "\tTrain Loss: 5.874 | Train PPL: 355.742\n",
      "\t Val. Loss: 6.525 |  Val. PPL: 682.282\n",
      "\tMetriques Val -> BLEU: 28.12 | chrF: 14.13 | METEOR: 2.63\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoque: 05 | Temps: 10.0m 9s \n",
      "\tTrain Loss: 5.809 | Train PPL: 333.226\n",
      "\t Val. Loss: 6.414 |  Val. PPL: 610.487\n",
      "\tMetriques Val -> BLEU: 6.32 | chrF: 9.06 | METEOR: 0.00\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoque: 06 | Temps: 10.0m 49s \n",
      "\tTrain Loss: 5.730 | Train PPL: 307.825\n",
      "\t Val. Loss: 6.471 |  Val. PPL: 645.991\n",
      "\tMetriques Val -> BLEU: 2.44 | chrF: 8.50 | METEOR: 1.11\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoque: 07 | Temps: 7.0m 45s \n",
      "\tTrain Loss: 5.668 | Train PPL: 289.582\n",
      "\t Val. Loss: 6.423 |  Val. PPL: 615.560\n",
      "\tMetriques Val -> BLEU: 1.52 | chrF: 1.60 | METEOR: 0.00\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoque: 08 | Temps: 8.0m 21s \n",
      "\tTrain Loss: 5.607 | Train PPL: 272.233\n",
      "\t Val. Loss: 6.447 |  Val. PPL: 630.643\n",
      "\tMetriques Val -> BLEU: 1.91 | chrF: 3.52 | METEOR: 0.00\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoque: 09 | Temps: 11.0m 31s \n",
      "\tTrain Loss: 5.561 | Train PPL: 260.029\n",
      "\t Val. Loss: 6.461 |  Val. PPL: 639.395\n",
      "\tMetriques Val -> BLEU: 11.21 | chrF: 11.29 | METEOR: 1.94\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoque: 10 | Temps: 11.0m 12s => Checkpoint sauvegarde !\n",
      "\tTrain Loss: 5.466 | Train PPL: 236.605\n",
      "\t Val. Loss: 6.382 |  Val. PPL: 591.028\n",
      "\tMetriques Val -> BLEU: 4.05 | chrF: 12.49 | METEOR: 1.54\n",
      "\n",
      "Entrainement termine. Meilleur modele sauvegarde sous : C:\\Users\\benic\\Documents\\Projet_DL_Translation\\models\\nmt_baseline_vanilla.pt\n"
     ]
    }
   ],
   "source": [
    "# Cellule 4 : Instanciation et Entrainement du modele Vanilla Seq2Seq\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Hyperparametres\n",
    "INPUT_DIM = 8000\n",
    "OUTPUT_DIM = 8000\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# Creation de l'architecture\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM, DROPOUT)\n",
    "model = Seq2SeqVanilla(enc, dec, device).to(device)\n",
    "\n",
    "# Optimiseur et Fonction de perte (on ignore l'ID du padding pour ne pas fausser la loss)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "PAD_IDX = 0 # sp_en.pad_id()\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1.0\n",
    "SAVE_PATH = str(ROOT_DIR / 'models' / 'nmt_baseline_vanilla.pt')\n",
    "\n",
    "# Lancement de la boucle avec Early Stopping manuel base sur la Validation Loss\n",
    "train_model(model, train_iterator, valid_iterator, optimizer, criterion, train_data.sp_en, N_EPOCHS, CLIP, SAVE_PATH, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2934e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demarrage de la traduction du set de Test...\n",
      "Inference terminee ! Resultats stockes dans le DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Cellule 5 : Inference sur le set de TEST (Scelle jusqu'a present)\n",
    "# Cette cellule calcule les predictions et les stocke. Elle est longue, ne la lancer qu'une fois !\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Chargement strict du meilleur checkpoint\n",
    "model.load_state_dict(torch.load(SAVE_PATH, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Chargement direct de SentencePiece pour l'inference pure\n",
    "sp_fr = spm.SentencePieceProcessor()\n",
    "sp_fr.load(spm_fr_path)\n",
    "sp_en = spm.SentencePieceProcessor()\n",
    "sp_en.load(spm_en_path)\n",
    "\n",
    "test_csv_path = str(ROOT_DIR / 'data' / 'processed' / 'test_nmt_fr_en.csv')\n",
    "df_test = pd.read_csv(test_csv_path)\n",
    "predictions = []\n",
    "\n",
    "def translate_sentence(sentence, model, sp_fr, sp_en, device, max_len=50):\n",
    "    model.eval()\n",
    "    tokens = [sp_fr.bos_id()] + sp_fr.encode_as_ids(sentence) + [sp_fr.eos_id()]\n",
    "    src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(src_tensor)\n",
    "        \n",
    "    trg_indexes = [sp_en.bos_id()]\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
    "            \n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indexes.append(pred_token)\n",
    "        if pred_token == sp_en.eos_id():\n",
    "            break\n",
    "            \n",
    "    trg_indexes = [t for t in trg_indexes if t not in [sp_en.bos_id(), sp_en.eos_id(), sp_en.pad_id()]]\n",
    "    return sp_en.decode_ids(trg_indexes)\n",
    "\n",
    "print(\"Demarrage de la traduction du set de Test...\")\n",
    "for idx, row in df_test.iterrows():\n",
    "    pred = translate_sentence(str(row['text_fr']), model, sp_fr, sp_en, device)\n",
    "    predictions.append(pred)\n",
    "\n",
    "df_test['prediction'] = predictions\n",
    "print(\"Inference terminee ! Resultats stockes dans le DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77192be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_fr</th>\n",
       "      <th>text_en</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>En réalité, la composition des roches, va nous permettre de savoir dans quelles conditions elle s'est formée.</td>\n",
       "      <td>In fact, the composition of the rocks helps us to know the conditions in which they were formed.</td>\n",
       "      <td>The,,,,,,,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Comme nous l'a dit Darwin, s'adapter ou mourir.</td>\n",
       "      <td>As Darwin told us, we should adapt or perish.</td>\n",
       "      <td>The,,,,,,,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>Et il n'a pas eu de chance.</td>\n",
       "      <td>And was unfortunate.</td>\n",
       "      <td>And then, I,,,,,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>Première idée fausse : ça coûte super cher de traverser ; avec tout cet argent, ils n'auraient pas pu faire autre chose ?</td>\n",
       "      <td>First misconception: passing through is expensive; with all this money, couldn't they do something else?</td>\n",
       "      <td>The,,,,,,,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>C'est la seule femme du groupe s'il vous plaît.</td>\n",
       "      <td>She is the only woman in the band.</td>\n",
       "      <td>It's a, of the,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>Je suis fier de moi. »</td>\n",
       "      <td>I'm proud of myself.\"</td>\n",
       "      <td>I'm going to to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>Le septième continent, il est dans la mer, c'est des particules de plastique qui, petit à petit, ruissellent et vont créer la plus grande décharge mondiale.</td>\n",
       "      <td>The seventh continent is in the sea, made of plastic particles that flow slowly, and will create the biggest garbage dump in the world.</td>\n",
       "      <td>The,,,,,,,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>Il m'a dit : « Chaque jour, je vais y retourner parce qu'ici, je suis déjà mort. »</td>\n",
       "      <td>He told me, \"Each day, I will go back because here I am already dead.\"</td>\n",
       "      <td>It's a, of the,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>Vous me direz : « Certains sont plus doués que d'autres. »</td>\n",
       "      <td>You may say, \"Some people are more gifted than others.\"</td>\n",
       "      <td>So, I,,,,,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>C'est formidable, non ?</td>\n",
       "      <td>It's amazing, isn't it?</td>\n",
       "      <td>It's a, of the,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                          text_fr  \\\n",
       "653                                                 En réalité, la composition des roches, va nous permettre de savoir dans quelles conditions elle s'est formée.   \n",
       "99                                                                                                                Comme nous l'a dit Darwin, s'adapter ou mourir.   \n",
       "987                                                                                                                                   Et il n'a pas eu de chance.   \n",
       "954                                     Première idée fausse : ça coûte super cher de traverser ; avec tout cet argent, ils n'auraient pas pu faire autre chose ?   \n",
       "309                                                                                                               C'est la seule femme du groupe s'il vous plaît.   \n",
       "501                                                                                                                                        Je suis fier de moi. »   \n",
       "240  Le septième continent, il est dans la mer, c'est des particules de plastique qui, petit à petit, ruissellent et vont créer la plus grande décharge mondiale.   \n",
       "977                                                                            Il m'a dit : « Chaque jour, je vais y retourner parce qu'ici, je suis déjà mort. »   \n",
       "528                                                                                                    Vous me direz : « Certains sont plus doués que d'autres. »   \n",
       "687                                                                                                                                       C'est formidable, non ?   \n",
       "\n",
       "                                                                                                                                     text_en  \\\n",
       "653                                         In fact, the composition of the rocks helps us to know the conditions in which they were formed.   \n",
       "99                                                                                             As Darwin told us, we should adapt or perish.   \n",
       "987                                                                                                                     And was unfortunate.   \n",
       "954                                 First misconception: passing through is expensive; with all this money, couldn't they do something else?   \n",
       "309                                                                                                       She is the only woman in the band.   \n",
       "501                                                                                                                    I'm proud of myself.\"   \n",
       "240  The seventh continent is in the sea, made of plastic particles that flow slowly, and will create the biggest garbage dump in the world.   \n",
       "977                                                                   He told me, \"Each day, I will go back because here I am already dead.\"   \n",
       "528                                                                                  You may say, \"Some people are more gifted than others.\"   \n",
       "687                                                                                                                  It's amazing, isn't it?   \n",
       "\n",
       "            prediction  \n",
       "653        The,,,,,,,,  \n",
       "99         The,,,,,,,,  \n",
       "987  And then, I,,,,,,  \n",
       "954        The,,,,,,,,  \n",
       "309    It's a, of the,  \n",
       "501    I'm going to to  \n",
       "240        The,,,,,,,,  \n",
       "977    It's a, of the,  \n",
       "528        So, I,,,,,,  \n",
       "687    It's a, of the,  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cellule 6 : Tableau comparatif\n",
    "# Vous pouvez relancer cette cellule autant de fois que vous voulez pour voir d'autres exemples\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df_test[['text_fr', 'text_en', 'prediction']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7304ca83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ANALYSE DE ROBUSTESSE ---\n",
      "BLEU Global : 0.18\n",
      "BLEU Phrases Courtes (<10 mots) : 0.25 (Support: 394)\n",
      "BLEU Phrases Longues (>25 mots) : 0.03 (Support: 152)\n",
      "\n",
      "Conclusion: L'effondrement du score sur les longues phrases demontre la perte d'information dans le vecteur de contexte unique.\n"
     ]
    }
   ],
   "source": [
    "# Cellule 7 : Preuve mathematique de la limite du modele Vanilla\n",
    "# Calcul des longueurs des phrases sources\n",
    "df_test['src_len'] = df_test['text_fr'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "df_short = df_test[df_test['src_len'] < 10]\n",
    "df_long = df_test[df_test['src_len'] > 25]\n",
    "\n",
    "def compute_bleu(df):\n",
    "    if len(df) == 0: return 0.0\n",
    "    preds = df['prediction'].tolist()\n",
    "    # SacreBLEU attend une liste de listes pour les references\n",
    "    refs = [df['text_en'].tolist()]\n",
    "    return sacrebleu.corpus_bleu(preds, refs).score\n",
    "\n",
    "print(f\"--- ANALYSE DE ROBUSTESSE ---\")\n",
    "print(f\"BLEU Global : {compute_bleu(df_test):.2f}\")\n",
    "print(f\"BLEU Phrases Courtes (<10 mots) : {compute_bleu(df_short):.2f} (Support: {len(df_short)})\")\n",
    "print(f\"BLEU Phrases Longues (>25 mots) : {compute_bleu(df_long):.2f} (Support: {len(df_long)})\")\n",
    "print(\"\\nConclusion: L'effondrement du score sur les longues phrases demontre la perte d'information dans le vecteur de contexte unique.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_dl (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
