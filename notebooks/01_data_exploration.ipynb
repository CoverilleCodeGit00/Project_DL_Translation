{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14c8c960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# On remonte d'un cran pour atteindre la racine du projet et accéder à /src\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "# Maintenant tu peux importer tes fonctions de l'Agent 1\n",
    "from src.data_utils import MTEDxParser, AudioProcessor\n",
    "\n",
    "# Pour que le notebook recharge automatiquement le code si tu modifies le fichier dans src\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21840725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction terminée dans C:\\Users\\benic\\Documents\\Projet_DL_Translation\\data\\temp (9 fichiers).\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from src.data_utils import MTEDxParser, AudioProcessor\n",
    "\n",
    "# Chemins Robustes\n",
    "ROOT_DIR = Path(\"..\").resolve()\n",
    "DATA_RAW = ROOT_DIR / \"data\" / \"raw\"\n",
    "DATA_TEMP = ROOT_DIR / \"data\" / \"temp\"\n",
    "DATA_PROCESSED = ROOT_DIR / \"data\" / \"processed\"\n",
    "\n",
    "# Initialisation\n",
    "parser = MTEDxParser(base_path=DATA_TEMP)\n",
    "\n",
    "# 1. Extraction sélective (Uniquement le français)\n",
    "archive_fr = DATA_RAW / \"mtedx_fr.tgz\"\n",
    "if archive_fr.exists():\n",
    "    parser.selective_extract(archive_fr, DATA_TEMP)\n",
    "else:\n",
    "    print(f\"Erreur : Archive manquante à l'emplacement {archive_fr}\")\n",
    "\n",
    "# 2. Chargement des métadonnées du split 'train' pour analyse\n",
    "# Note : on passe lang_pair=\"fr-fr\" car c'est la structure interne de l'archive\n",
    "df_segments, train_meta = parser.parse_metadata(split=\"train\", lang_pair=\"fr-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8dfc5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ANALYSE DU DATASET (TRAIN) ---\n",
      "Total de segments de parole : 116045\n",
      "Total de fichiers vidéos sources détectés : 949\n",
      "\n",
      "Exemple d'une entrée YAML :\n",
      "{'duration': 0.879999999999999, 'offset': 8.57, 'speaker_id': '0-DmpK7Di0c', 'wav': '0-DmpK7Di0c.wav'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if train_meta:\n",
    "    # 1. Nombre total de segments\n",
    "    total_segments = len(train_meta)\n",
    "    \n",
    "    # 2. Nombre de vidéos uniques (via le fichier segments corrélé)\n",
    "    unique_videos = df_segments['video_id'].nunique()\n",
    "    \n",
    "    print(f\"--- ANALYSE DU DATASET (TRAIN) ---\")\n",
    "    print(f\"Total de segments de parole : {total_segments}\")\n",
    "    print(f\"Total de fichiers vidéos sources détectés : {unique_videos}\")\n",
    "    \n",
    "    # 3. Aperçu de la structure YAML pour l'Agent MODEL\n",
    "    print(\"\\nExemple d'une entrée YAML :\")\n",
    "    print(train_meta[0]) \n",
    "    # Typiquement : {'duration': 4.2, 'offset': 10.5, 'text': '...', 'utt_id': '...'}\n",
    "else:\n",
    "    print(\"Métadonnées non chargées. Vérifiez l'extraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "942a7716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction d'un échantillon depuis : 0-DmpK7Di0c.wav\n",
      "Échantillon prêt. Lecture en cours...\n",
      "Erreur lors de l'extraction : rate must be specified when data is a numpy array or list of audio samples.\n",
      "Note : Vérifiez que le fichier .wav a bien été extrait dans data/temp/.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "# Choix du premier segment pour le test\n",
    "sample_seg = df_segments.iloc[0]\n",
    "video_id = sample_seg['video_id']\n",
    "# Chemin vers le dossier wav original (attention : nécessite que le .wav soit accessible)\n",
    "path_to_wav = DATA_TEMP / \"fr-fr\" / \"data\" / \"train\" / \"wav\" / f\"{video_id}.wav\"\n",
    "output_sample = DATA_PROCESSED / \"test_sample.wav\"\n",
    "\n",
    "print(f\"Extraction d'un échantillon depuis : {video_id}.wav\")\n",
    "\n",
    "# On utilise notre AudioProcessor pour standardiser (16kHz, Mono)\n",
    "# On force une durée de 10s pour le test (si la vidéo est assez longue)\n",
    "try:\n",
    "    AudioProcessor.standardize_audio(\n",
    "        input_path=path_to_wav,\n",
    "        output_path=output_sample,\n",
    "        start=sample_seg['start'], \n",
    "        duration=10.0\n",
    "    )\n",
    "    \n",
    "    print(\"Échantillon prêt. Lecture en cours...\")\n",
    "    display(Audio(str(output_sample)))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de l'extraction : {e}\")\n",
    "    print(\"Note : Vérifiez que le fichier .wav a bien été extrait dans data/temp/.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1102554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def plot_segment_diagnostic(audio_path: Path, title: str = \"Diagnostic Audio\"):\n",
    "    \"\"\"Trace la forme d'onde et vérifie la saturation/silence.\"\"\"\n",
    "    if not audio_path.exists():\n",
    "        print(f\"Erreur : Fichier introuvable {audio_path}\")\n",
    "        return\n",
    "\n",
    "    # Chargement (Librosa normalise automatiquement entre -1.0 et 1.0)\n",
    "    y, sr = librosa.load(audio_path, sr=16000)\n",
    "    \n",
    "    duration = librosa.get_duration(y=y, sr=sr)\n",
    "    max_amp = np.max(np.abs(y))\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    librosa.display.waveshow(y, sr=sr, color='#1f77b4', alpha=0.8)\n",
    "    \n",
    "    # Indicateurs de saturation (Threshold à 0.99 pour détecter l'écrêtage)\n",
    "    plt.axhline(y=1.0, color='r', linestyle='--', alpha=0.5, label='Plafond de Saturation')\n",
    "    plt.axhline(y=-1.0, color='r', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Analyse rapide\n",
    "    status = \"OK\"\n",
    "    if max_amp >= 0.99:\n",
    "        status = \"⚠️ SATURATION DÉTECTÉE\"\n",
    "        plt.gca().set_facecolor('#fff5f5')\n",
    "    elif max_amp < 0.01:\n",
    "        status = \"⚠️ SEGMENT VIDE OU SILENCIEUX\"\n",
    "        plt.gca().set_facecolor('#f0f0f0')\n",
    "\n",
    "    plt.title(f\"{title} | État : {status} | Durée : {duration:.2f}s\")\n",
    "    plt.xlabel(\"Temps (secondes)\")\n",
    "    plt.ylabel(\"Amplitude (Normalisée)\")\n",
    "    plt.ylim(-1.1, 1.1)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True, linestyle=':', alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "# Exemple d'utilisation après segmentation\n",
    "# plot_segment_diagnostic(Path(\"data/processed/segment_001.wav\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_dl (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
